# Install required packages
!pip install transformers torch gradio accelerate

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class GraniteAIApp:
    def __init__(self):
        self.model_name = "ibm-granite/granite-3.2-2b-instruct"
        self.tokenizer = None
        self.model = None
        self.load_model()
    
    def load_model(self):
        """Load the Granite model and tokenizer"""
        print("Loading Granite 3.2-2B Flash model...")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # Load model with appropriate settings for Colab
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        print("Model loaded successfully!")
    
    def generate_response(self, user_input, max_length=512, temperature=0.7, top_p=0.9):
        """Generate AI response based on user input"""
        if not user_input.strip():
            return "Please enter a valid input."
        
        try:
            # Format input for instruction-following
            formatted_input = f"<|user|>\n{user_input}\n<|assistant|>\n"
            
            # Tokenize input
            inputs = self.tokenizer(formatted_input, return_tensors="pt")
            
            # Move to GPU if available
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the assistant's response
            if "<|assistant|>" in response:
                response = response.split("<|assistant|>")[-1].strip()
            
            return response
            
        except Exception as e:
            return f"Error generating response: {str(e)}"

# Initialize the AI application
ai_app = GraniteAIApp()

def chat_interface(message, history, max_length, temperature, top_p):
    """Chat interface function for Gradio"""
    response = ai_app.generate_response(
        message, 
        max_length=int(max_length), 
        temperature=temperature, 
        top_p=top_p
    )
    
    # Update conversation history
    history.append((message, response))
    return history, ""

# Create Gradio interface
with gr.Blocks(title="Granite AI Assistant", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ü§ñ Granite 3.2-2B AI Assistant")
    gr.Markdown("Powered by IBM Granite 3.2-2B Flash model")
    
    with gr.Row():
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(
                label="Conversation",
                height=400,
                show_label=True
            )
            
            msg = gr.Textbox(
                label="Your Message",
                placeholder="Type your message here...",
                lines=2
            )
            
            with gr.Row():
                send_btn = gr.Button("Send", variant="primary")
                clear_btn = gr.Button("Clear Chat", variant="secondary")
        
        with gr.Column(scale=1):
            gr.Markdown("### ‚öôÔ∏è Generation Settings")
            
            max_length = gr.Slider(
                label="Max Length",
                minimum=100,
                maximum=1000,
                value=512,
                step=50
            )
            
            temperature = gr.Slider(
                label="Temperature",
                minimum=0.1,
                maximum=2.0,
                value=0.7,
                step=0.1
            )
            
            top_p = gr.Slider(
                label="Top P",
                minimum=0.1,
                maximum=1.0,
                value=0.9,
                step=0.05
            )
    
    # Event handlers
    send_btn.click(
        chat_interface,
        inputs=[msg, chatbot, max_length, temperature, top_p],
        outputs=[chatbot, msg]
    )
    
    msg.submit(
        chat_interface,
        inputs=[msg, chatbot, max_length, temperature, top_p],
        outputs=[chatbot, msg]
    )
    
    clear_btn.click(lambda: ([], ""), outputs=[chatbot, msg])

# Launch the application
if __name__ == "__main__":
    demo.launch(
        share=True,  # Creates public link for sharing
        debug=True,
        server_name="0.0.0.0",
        server_port=7860
    )